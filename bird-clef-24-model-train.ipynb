{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tf-models-official==2.15\n!pip install keras==2.15\n!pip install tensorflow-addons==0.23.0\n!pip install vit-keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nfrom joblib import Parallel, delayed\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nimport tensorflow as tf\nfrom tensorflow_addons.optimizers import RectifiedAdam\nfrom tensorflow.keras.layers import Conv2D, UpSampling2D, Input, TimeDistributed, Dropout, BatchNormalization, Dense\nfrom tensorflow.keras.models import Model\nfrom vit_keras import vit\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T13:11:50.391568Z","iopub.execute_input":"2024-06-09T13:11:50.391971Z","iopub.status.idle":"2024-06-09T13:11:50.398472Z","shell.execute_reply.started":"2024-06-09T13:11:50.391941Z","shell.execute_reply":"2024-06-09T13:11:50.397565Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def extract_audio(filename, sr=16000):\n    filepath = '/kaggle/input/birdclef-2024/train_audio/' + filename\n    audio, _ = librosa.load(filepath, sr=sr)\n    return audio.astype(np.float16)\n\ndef pad_array(array):\n    target_dim = 98\n    current_dim = array.shape[0]\n    if current_dim < target_dim:\n        padding_amount = target_dim - current_dim\n        padding_shape = (padding_amount,) + array.shape[1:]\n        padding_array = np.zeros(padding_shape, dtype=array.dtype)\n        padded_array = np.concatenate((array, padding_array), axis=0)\n        if len(padded_array.shape)==1: print(array.shape, padding_array.shape)\n        return padded_array.astype(np.float16)\n    else:\n        truncated_array = array[:target_dim, ...]\n        return truncated_array.astype(np.float16)\n\ndef augment_data(audio_array, sr, n_fft):\n    audio_array = librosa.effects.pitch_shift(y=audio_array.astype(np.float32), sr=sr,\n                                              n_steps=np.random.uniform(low=.970, high=1.03),\n                                              n_fft=n_fft)\n    audio_array = librosa.effects.time_stretch(y=audio_array,\n                                               rate=np.random.uniform(low=.975, high=1.025)) #n_fft=16384\n    noise_amp = np.random.normal(loc=0.0, scale=0.005)*np.amax(audio_array)\n    audio_array = audio_array + noise_amp*np.random.normal(size=audio_array.shape[0])\n    shift_range = int(np.random.uniform(low=-5, high=5)*sr*10)\n    audio_array = np.roll(audio_array, shift_range)\n    return audio_array.astype(np.float16)\n\n# Function to generate augmented data\ndef generate_augmented_data(df, column, label_column, threshold, sr=16000, n_fft=2048):\n    augmented_data = []\n    for label, count in df[label_column].value_counts().items():\n        if count < threshold:\n            # Select rows with this label\n            num_needed = threshold-count\n            label_df = df[df[label_column] == label]\n            num_samples = min(num_needed, len(label_df))\n            label_df = label_df.sample(n= num_samples,\n                                        random_state=random.randint(0, 10000))\n\n            for _, row in label_df.iterrows():\n                npy_array = row[column]\n                augmented_npy = augment_data(npy_array, sr, n_fft)\n                augmented_row = row.copy()\n                augmented_row[column] = augmented_npy\n                augmented_row['augmented'] = True\n                augmented_data.append(augmented_row)\n    # Append the augmented data to the original dataframe\n    augmented_df = pd.DataFrame(augmented_data)\n    result_df = pd.concat([df, augmented_df], ignore_index=True)\n    return result_df\n\ndef trim_df_by_label(df, label_column, threshold):\n    # Create an empty DataFrame to store the results\n    trimmed_df = pd.DataFrame(columns=df.columns)\n    # Group by the label column\n    grouped = df.groupby(label_column)\n    for label, group in grouped:\n        if len(group) > threshold:\n            sampled_group = group.sample(n=threshold, random_state=1)\n        else:\n            sampled_group = group\n        \n        trimmed_df = pd.concat([trimmed_df, sampled_group])\n    return trimmed_df\n\ndef extract_framed(audio):\n    \"\"\"\n    Load audio files from the given DataFrame, extract framed audios,\n    and add the framed audios to the given DataFrame\n\n    Parameters:\n      dataframe (pd.DataFrame): DataFrame containing audio file information.\n      augment (bool): Whether to apply augmentation to the audio.\n    \"\"\"\n    window_size_s=2.5\n    hop_size_s=2.5\n    sr=16000\n    frame_length = int(window_size_s * sr)\n    frame_step = int(hop_size_s * sr)\n    return tf.cast(tf.signal.frame(audio, frame_length, frame_step, pad_end=True), tf.float16)\n\ndef compute_mfcc(frame, sr, n_mfcc, hop_length):\n    return librosa.feature.mfcc(y=frame, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n\ndef compute_rms(frame, hop_length):\n    return librosa.feature.rms(y=frame, hop_length=hop_length)\n\ndef extract_features(row):\n    n_mfcc=33\n    sr=16000\n    hop_length=626\n    frames = row['npy'].numpy().astype(np.float32)\n    mfcc = Parallel(n_jobs=-1, backend='threading')(delayed(compute_mfcc)(frame, sr, n_mfcc, hop_length) for frame in frames)\n    mfcc = np.array(mfcc).astype(np.float16)\n    rms = Parallel(n_jobs=-1, backend='threading')(delayed(compute_rms)(frame, hop_length) for frame in frames)\n    rms = np.array(rms).astype(np.float16)\n    row['npy'] = None\n    return pd.Series({'mfcc': mfcc, 'rms': rms})\n\n# Function to normalize the entire column data using Min-Max Normalization\ndef normalize_column(data, scaler):\n    stacked = np.concatenate(data)\n    original_shape = stacked.shape\n    reshaped_stacked = stacked.reshape(-1, original_shape[-1])\n    normalized_reshaped = scaler.fit_transform(reshaped_stacked)\n    normalized_stacked = normalized_reshaped.reshape(original_shape).astype(np.float16)\n    split_indices = np.cumsum([arr.shape[0] for arr in data[:-1]])\n    normalized_data = np.split(normalized_stacked, split_indices)\n    return normalized_data\n\n# Function to concatenate numpy arrays along a specified axis and replace original values with None\ndef concat_and_replace(row, axis=0):\n    reshaped_long = np.full(row['rms'].shape, row['longitude'])\n    reshaped_lat = np.full(row['rms'].shape, row['latitude'])\n    concatenated_data = np.concatenate((row['rms'], reshaped_long), axis=-2)\n    concatenated_data = np.concatenate((concatenated_data, reshaped_lat), axis=-2)\n    concatenated_data = np.concatenate((row['mfcc'], concatenated_data), axis=-2)\n    concatenated_data = np.transpose(concatenated_data, (0, 2, 1))\n    concatenated_data = np.expand_dims(concatenated_data, axis=-1)\n    row['rms'] = None\n    row['mfcc'] = None\n    row['longitude'] = None\n    row['latitude'] = None    \n    return concatenated_data.astype(np.float16)\n\ndef birds_stratified_split(df, target_col, test_size=0.2):\n    class_counts = df[target_col].value_counts()\n    low_count_classes = class_counts[class_counts < 2].index.tolist() ### Birds with single counts\n    df['train'] = df[target_col].isin(low_count_classes)\n    train_df, test_df = train_test_split(df[~df['train']], test_size=test_size, stratify=df[~df['train']][target_col], random_state=42)\n    train_df = pd.concat([train_df, df[df['train']]], axis=0).reset_index(drop=True)\n    # Remove the 'valid' column\n    train_df.drop('train', axis=1, inplace=True)\n    test_df.drop('train', axis=1, inplace=True)\n\n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:09:43.975744Z","iopub.execute_input":"2024-06-09T13:09:43.976266Z","iopub.status.idle":"2024-06-09T13:09:44.009413Z","shell.execute_reply.started":"2024-06-09T13:09:43.976233Z","shell.execute_reply":"2024-06-09T13:09:44.008467Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ViT = vit.vit_b16(\n    image_size=224,\n    activation='sigmoid',\n    pretrained=True,\n    include_top=True,\n    pretrained_top=True,\n    classes=182\n)\n\ninput_shape = (98, 64, 36, 1)\ninputs = Input(shape=(input_shape))\nbatch_size = 20\nx = TimeDistributed(Conv2D(30, (9, 9), padding='valid', activation='relu', strides=(2, 1)))(inputs)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = TimeDistributed(Conv2D(6, (13, 13), padding='valid', activation='tanh', strides=(1, 1)))(x)\nx = Dropout(0.2)(x)\nx = tf.keras.layers.Reshape((224, 224, 3))(x)\nx = ViT(x)\nx = tf.keras.layers.Dense(384, activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = Dense(79, activation = 'softmax')(x)\nmodel = Model(inputs=inputs, outputs=x)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:09:44.011478Z","iopub.execute_input":"2024-06-09T13:09:44.011839Z","iopub.status.idle":"2024-06-09T13:09:52.730282Z","shell.execute_reply.started":"2024-06-09T13:09:44.011807Z","shell.execute_reply":"2024-06-09T13:09:52.729405Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/vit_keras/vit.py:139: UserWarning: Can only use pretrained_top with imagenet21k+imagenet2012 if classes = 1000. Setting manually.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n347502902/347502902 [==============================] - 1s 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 98, 64, 36, 1)]   0         \n                                                                 \n time_distributed (TimeDist  (None, 98, 28, 28, 30)    2460      \n ributed)                                                        \n                                                                 \n dropout (Dropout)           (None, 98, 28, 28, 30)    0         \n                                                                 \n batch_normalization (Batch  (None, 98, 28, 28, 30)    120       \n Normalization)                                                  \n                                                                 \n time_distributed_1 (TimeDi  (None, 98, 16, 16, 6)     30426     \n stributed)                                                      \n                                                                 \n dropout_1 (Dropout)         (None, 98, 16, 16, 6)     0         \n                                                                 \n reshape_1 (Reshape)         (None, 224, 224, 3)       0         \n                                                                 \n vit-b16 (Functional)        (None, 1000)              86567656  \n                                                                 \n dense (Dense)               (None, 384)               384384    \n                                                                 \n batch_normalization_1 (Bat  (None, 384)               1536      \n chNormalization)                                                \n                                                                 \n dense_1 (Dense)             (None, 79)                30415     \n                                                                 \n=================================================================\nTotal params: 87016997 (331.94 MB)\nTrainable params: 87016169 (331.94 MB)\nNon-trainable params: 828 (3.23 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# # instantiate a distribution strategy\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n\n# # instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\n#     model = tf.keras.Sequential( … ) # define your model normally\n#     model.compile( … )\n\n# # train model normally\n# model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_df = pd.read_csv('/kaggle/input/filtered-df-final/filtered_df_final.csv')\nprint(filtered_df['primary_label'].nunique())\nprint(len(filtered_df))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:09:52.731584Z","iopub.execute_input":"2024-06-09T13:09:52.731927Z","iopub.status.idle":"2024-06-09T13:09:52.896478Z","shell.execute_reply.started":"2024-06-09T13:09:52.731895Z","shell.execute_reply":"2024-06-09T13:09:52.895528Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"79\n18295\n","output_type":"stream"}]},{"cell_type":"code","source":"# value_counts = filtered_df['primary_label'].value_counts()\n# filtered_categories = value_counts[value_counts > 80].index\n# filtered_df = filtered_df[filtered_df['primary_label'].isin(filtered_categories)]\n# filtered_df = filtered_df.sample(frac=1).reset_index(drop=True)\n\n# print(filtered_df['primary_label'].nunique())\n# print(len(filtered_df))\n\n# filtered_df.to_csv('/kaggle/working/filtered_df_final.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T12:56:56.277265Z","iopub.execute_input":"2024-06-09T12:56:56.277700Z","iopub.status.idle":"2024-06-09T12:56:56.666857Z","shell.execute_reply.started":"2024-06-09T12:56:56.277656Z","shell.execute_reply":"2024-06-09T12:56:56.665706Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"79\n18295\n","output_type":"stream"}]},{"cell_type":"code","source":"# filtered_df = filtered_df.sample(frac=1).reset_index(drop=True)\n\n######################################################################\n# interested_species = ['grewar3', 'asikoe2', 'commyn', 'rorpar'] #'blrwar1', 'woosan', 'grnsan'\n# filtered_df = filtered_df[filtered_df['primary_label'].isin(interested_species)]\n\ndf = filtered_df[['primary_label', 'latitude', 'longitude', 'filename']]\nprint(f\"num of species: {df['primary_label'].nunique()}\")\ninitial_length = len(df)\nlabel_counts = df['primary_label'].value_counts().to_dict()\nnew_df = pd.DataFrame(columns=[df.columns])\n\nlearning_rate = 1e-4\noptimizer = RectifiedAdam(lr=learning_rate)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy', metrics=['accuracy'])\nearly_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True, verbose = 1)\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n# Fit and transform the labels\n_ = encoder.fit_transform(df['primary_label'].values.reshape(-1, 1))\nlong_lat_scaler = MinMaxScaler(feature_range=(-1, 1))\n_ = long_lat_scaler.fit_transform(df['longitude'].values.reshape(-1, 1))\n\n# Path to save models\noutput_dir = '/kaggle/working/saved_models/'\nos.makedirs(output_dir, exist_ok=True)\n\ntrain_count = 0\n# While current DataFrame length is greater than a certain value\nwhile len(df) > initial_length * 0.005:  # For example, continue until half of the initial length\n    start_time = time.time()  # Record start time\n    print(f\"trained data count: {train_count}\")\n    print(f\"progress through data: {(train_count/initial_length)*100}%\")\n    new_df = df.sample(frac=1).reset_index(drop=True)\n    new_df = pd.DataFrame(columns=df.columns)\n    # Iterate through each key in the stored frequency dictionary\n    for label, count in label_counts.items():\n        # Extract 2% of the data with that value\n        sample_size = max(int(count * 0.02), 1)\n        extracted_data = df[df['primary_label'] == label].head(sample_size)\n        df = df.drop(extracted_data.index)\n        new_df = pd.concat([new_df, extracted_data])\n    # Keep count of trained data\n    train_count += len(new_df)\n    # Load, Augment, Pad and Trim Audio Data\n    new_df['npy'] = new_df['filename'].apply(extract_audio)\n    print(f\"extracted npy\")\n    new_df['augmented'] = False\n    threshold = min(max(new_df['primary_label'].value_counts().values.tolist()), 6)\n    print(f\"Threshold: {threshold}\")\n    while min(new_df['primary_label'].value_counts().values.tolist())<threshold:\n        new_df = generate_augmented_data(new_df, 'npy', 'primary_label', threshold, sr=16000, n_fft=2048)\n    new_df = trim_df_by_label(new_df, 'primary_label', threshold)\n    print(f\"augmented npy\")\n    new_df['npy'] = new_df['npy'].apply(extract_framed)\n    new_df[['mfcc', 'rms']] = new_df.apply(lambda row: extract_features(row), axis=1)\n    \n    new_df['mfcc'] = new_df['mfcc'].apply(pad_array)\n    new_df['rms'] = new_df['rms'].apply(pad_array)\n    print(f\"extracted mfcc and rms\")\n    new_df['mfcc'] = normalize_column(new_df['mfcc'].values, MinMaxScaler(feature_range=(-1, 1)))\n    new_df['rms'] = normalize_column(new_df['rms'].values, MinMaxScaler(feature_range=(-1, 1)))\n    new_df['longitude'] = long_lat_scaler.transform(new_df['longitude'].values.reshape(-1, 1)).astype(np.float16)\n    new_df['latitude'] = long_lat_scaler.transform(new_df['latitude'].values.reshape(-1, 1)).astype(np.float16)\n    new_df['features'] = new_df.apply(lambda row: concat_and_replace(row, axis=1), axis=1)\n    new_df = new_df[['primary_label', 'features']]\n    print(f\"ready to train\")\n    # Split Stratified Train and Test Sets\n    print(f\"Preprocessed data entries for training: {len(new_df)}\")\n    train_df, test_df = birds_stratified_split(new_df, 'primary_label', 0.3)\n    # Fit and transform the labels\n    train_labels = encoder.transform(train_df['primary_label'].values.reshape(-1, 1)).astype(np.float16)\n    test_labels = encoder.transform(test_df['primary_label'].values.reshape(-1, 1)).astype(np.float16)\n    # Convert features to numpy arrays\n    train_features = np.stack(train_df['features'].values)\n    test_features = np.stack(test_df['features'].values)\n    train_features = np.nan_to_num(train_features, nan=0)\n    test_features = np.nan_to_num(test_features, nan=0)\n    # Train The Model\n    history = model.fit(train_features, train_labels, epochs=40, batch_size=28, validation_data=(test_features, test_labels), callbacks = early_stopping_callbacks)\n    # Save the model after each chunk\n    model_save_path = os.path.join(output_dir, 'bird_clef_24_model.h5')\n    model.save(model_save_path)\n    print(f'Saved model to {model_save_path}')\n    end_time = time.time()  # Record end time\n    print(f'Time taken for prev iteration: {(end_time - start_time)/60} mins')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:11:58.360002Z","iopub.execute_input":"2024-06-09T13:11:58.360426Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"num of species: 79\ntrained data count: 0\nprogress through data: 0.0%\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_addons/optimizers/rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super().__init__(name, **kwargs)\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/tmp/ipykernel_34/2787601871.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  new_df = pd.concat([new_df, extracted_data])\n","output_type":"stream"},{"name":"stdout","text":"extracted npy\nThreshold: 6\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/5713270.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  trimmed_df = pd.concat([trimmed_df, sampled_group])\n","output_type":"stream"},{"name":"stdout","text":"augmented npy\nextracted mfcc and rms\nready to train\nPreprocessed data entries for training: 474\nEpoch 1/40\n12/12 [==============================] - 53s 2s/step - loss: 4.7096 - accuracy: 0.0151 - val_loss: 4.5189 - val_accuracy: 0.0140\nEpoch 2/40\n12/12 [==============================] - 10s 861ms/step - loss: 4.6890 - accuracy: 0.0242 - val_loss: 4.5078 - val_accuracy: 0.0140\nEpoch 3/40\n12/12 [==============================] - 10s 863ms/step - loss: 4.5498 - accuracy: 0.0272 - val_loss: 4.4958 - val_accuracy: 0.0140\nEpoch 4/40\n12/12 [==============================] - 10s 859ms/step - loss: 4.5497 - accuracy: 0.0211 - val_loss: 4.4863 - val_accuracy: 0.0140\nEpoch 5/40\n12/12 [==============================] - 10s 863ms/step - loss: 4.4889 - accuracy: 0.0151 - val_loss: 4.4799 - val_accuracy: 0.0140\nEpoch 6/40\n12/12 [==============================] - 10s 857ms/step - loss: 4.3423 - accuracy: 0.0272 - val_loss: 4.4730 - val_accuracy: 0.0140\nEpoch 7/40\n12/12 [==============================] - 10s 860ms/step - loss: 4.2988 - accuracy: 0.0302 - val_loss: 4.4662 - val_accuracy: 0.0000e+00\nEpoch 8/40\n12/12 [==============================] - 10s 859ms/step - loss: 4.2037 - accuracy: 0.0302 - val_loss: 4.4623 - val_accuracy: 0.0070\nEpoch 9/40\n12/12 [==============================] - 10s 860ms/step - loss: 4.1588 - accuracy: 0.0453 - val_loss: 4.4585 - val_accuracy: 0.0070\nEpoch 10/40\n12/12 [==============================] - 10s 860ms/step - loss: 4.0764 - accuracy: 0.0363 - val_loss: 4.4572 - val_accuracy: 0.0070\nEpoch 11/40\n12/12 [==============================] - 10s 841ms/step - loss: 4.0085 - accuracy: 0.0242 - val_loss: 4.4608 - val_accuracy: 0.0070\nEpoch 12/40\n12/12 [==============================] - 10s 860ms/step - loss: 4.0065 - accuracy: 0.0272 - val_loss: 4.4563 - val_accuracy: 0.0070\nEpoch 13/40\n12/12 [==============================] - 10s 840ms/step - loss: 3.9473 - accuracy: 0.0393 - val_loss: 4.4570 - val_accuracy: 0.0070\nEpoch 14/40\n12/12 [==============================] - 10s 839ms/step - loss: 3.8867 - accuracy: 0.0514 - val_loss: 4.4594 - val_accuracy: 0.0070\nEpoch 15/40\n12/12 [==============================] - 10s 841ms/step - loss: 3.8578 - accuracy: 0.0423 - val_loss: 4.4602 - val_accuracy: 0.0070\nEpoch 16/40\n12/12 [==============================] - 10s 840ms/step - loss: 3.7788 - accuracy: 0.0574 - val_loss: 4.4638 - val_accuracy: 0.0070\nEpoch 17/40\n12/12 [==============================] - 10s 843ms/step - loss: 3.7722 - accuracy: 0.0604 - val_loss: 4.4672 - val_accuracy: 0.0070\nEpoch 18/40\n12/12 [==============================] - 10s 841ms/step - loss: 3.8059 - accuracy: 0.0423 - val_loss: 4.4739 - val_accuracy: 0.0000e+00\nEpoch 19/40\n12/12 [==============================] - 10s 841ms/step - loss: 3.7225 - accuracy: 0.0846 - val_loss: 4.4761 - val_accuracy: 0.0070\nEpoch 20/40\n12/12 [==============================] - 10s 842ms/step - loss: 3.7305 - accuracy: 0.0695 - val_loss: 4.4843 - val_accuracy: 0.0210\nEpoch 21/40\n12/12 [==============================] - 10s 842ms/step - loss: 3.6767 - accuracy: 0.0937 - val_loss: 4.4963 - val_accuracy: 0.0210\nEpoch 22/40\n12/12 [==============================] - 10s 843ms/step - loss: 3.6998 - accuracy: 0.0816 - val_loss: 4.5089 - val_accuracy: 0.0070\nEpoch 23/40\n12/12 [==============================] - 10s 842ms/step - loss: 3.7422 - accuracy: 0.0574 - val_loss: 4.5345 - val_accuracy: 0.0210\nEpoch 24/40\n12/12 [==============================] - 10s 841ms/step - loss: 3.6766 - accuracy: 0.0785 - val_loss: 4.5532 - val_accuracy: 0.0280\nEpoch 25/40\n12/12 [==============================] - 10s 842ms/step - loss: 3.6949 - accuracy: 0.0604 - val_loss: 4.5787 - val_accuracy: 0.0210\nEpoch 26/40\n12/12 [==============================] - 10s 841ms/step - loss: 3.6745 - accuracy: 0.0816 - val_loss: 4.5690 - val_accuracy: 0.0350\nEpoch 27/40\n12/12 [==============================] - 10s 840ms/step - loss: 3.6493 - accuracy: 0.0544 - val_loss: 4.5697 - val_accuracy: 0.0280\nEpoch 28/40\n12/12 [==============================] - 10s 840ms/step - loss: 3.5634 - accuracy: 0.1057 - val_loss: 4.5784 - val_accuracy: 0.0280\nEpoch 29/40\n12/12 [==============================] - 10s 842ms/step - loss: 3.5760 - accuracy: 0.0906 - val_loss: 4.5713 - val_accuracy: 0.0210\nEpoch 30/40\n12/12 [==============================] - 10s 839ms/step - loss: 3.6580 - accuracy: 0.0846 - val_loss: 4.5589 - val_accuracy: 0.0490\nEpoch 31/40\n12/12 [==============================] - 10s 839ms/step - loss: 3.6787 - accuracy: 0.0514 - val_loss: 4.5818 - val_accuracy: 0.0420\nEpoch 32/40\n12/12 [==============================] - ETA: 0s - loss: 3.5104 - accuracy: 0.0725Restoring model weights from the end of the best epoch: 12.\n12/12 [==============================] - 10s 866ms/step - loss: 3.5104 - accuracy: 0.0725 - val_loss: 4.5913 - val_accuracy: 0.0420\nEpoch 32: early stopping\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"Saved model to /kaggle/working/saved_models/bird_clef_24_model.h5\nTime taken for prev iteration: 11.633314502239227 mins\ntrained data count: 326\nprogress through data: 1.7819076250341621%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2787601871.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  new_df = pd.concat([new_df, extracted_data])\n","output_type":"stream"},{"name":"stdout","text":"extracted npy\nThreshold: 6\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/5713270.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  trimmed_df = pd.concat([trimmed_df, sampled_group])\n","output_type":"stream"},{"name":"stdout","text":"augmented npy\n","output_type":"stream"}]}]}